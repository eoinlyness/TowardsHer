{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyGrid: A Peer-to-Peer Platform for Private Data Science and Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a simple example, this notebook introduces to beginners what PyGrid is and how you can use it for your data science projects. \n",
    "\n",
    "### Credits:\n",
    "- Original authors: \n",
    " - Emma Bluemke - Twitter: [@emmabluemke](https://twitter.com/emmabluemke) - Github: [@em-blue](@https://github.com/em-blue)\n",
    " - Ionesio Junior - Github: [@IonesioJunio](https://github.com/IonesioJunior)\n",
    "\n",
    "\n",
    "- Reviewers: \n",
    " - Patrick Cason - Github: [@cereallcerny](https://github.com/cereallarceny)\n",
    " - Juan M. Aunon - Twitter: [@jm_aunon](https://twitter.com/jm_aunon) - Github: [@jmaunon](https://github.com/jmaunon)\n",
    "\n",
    "\n",
    "- New Content tested and enriched by: \n",
    " - Dev Bharti - Github: [@devtastic](https://github.com/devtastic)\n",
    " - Nahua Kang - Github: [@nahuakang](https://github.com/nahuakang)\n",
    " \n",
    " \n",
    " _This notebook covers the first part of [what-is-pygrid-demo](https://blog.openmined.org/what-is-pygrid-demo/)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __What if you could train on all of the world’s data without that data leaving its device while keeping that data private?__\n",
    "\n",
    "PyGrid is a peer-to-peer platform for private data science and federated learning. With PyGrid, data owners can provide, monitor, and __manage access to their own private data clusters. The data does not leave the data owner’s server__.\n",
    "\n",
    "Data scientists and researchers can then use PyGrid to perform __privacy-preserving statistical analysis__ on the private dataset, or even perform __federated learning across multiple institution’s datasets__.\n",
    "\n",
    "### This blog post will cover:\n",
    "\n",
    "1. Basic concepts such as __Federated Learning__ and __Secure Multi-party Computation__, which are necessary for understanding __PyGrid__.\n",
    "2. The __PySyft__ library and the __PyGrid__ platform. PySyft is a privacy-preserving machine learning library that is deployed using the PyGrid platform.\n",
    "3. __Practical examples__ of privacy-preserving analysis using PyGrid, which offer a peek into PyGrid’s architecture and illustrate how you can apply PyGrid to real problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning\n",
    "\n",
    "Before embarking on our journey in PyGrid, w should understand the basics of Federated Learning. Federated Learning is a machine learning technique that trains AI models on data hosted across multiple decentralized servers and edge devices without exchanging the data. How does this work?\n",
    "\n",
    "First, the data scientist create an initial model. Then, they would send this model to a dataset owner - in this case, Joe’s device (see illustration below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Joe can update this model by training it on his dataset. After training completes, Joe returns the updated model, as in the new model weights, to the data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training on Joe's data, the data scientist decides to send the updated AI model to, say, Jane’s device. Therefore, Jane updates the data scientist's model by training it on her dataset. After training on her data completes, Jane sends the updated weights back to the data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data scientist's model has learned from both Joe and Jane’s data. But the story does not end here. In federated learning, the data scientist can repeat this process over many nodes, or even train models simultaneously on multiple nodes and average the updated weights, allowing for faster improvements to the model.\n",
    "\n",
    "The main benefits of federated learning are:\n",
    "\n",
    "- Training data stays on the host servers and edge devices (for example, the hospital servers or a user's smartphone)\n",
    "- Privacy-focused solution that enhances protection for sensitive data\n",
    "- Reduce legal liability for model owners (i.e. the data scientists or companies)\n",
    "- Reduce network bandwidth involved in uploading large datasets for training\n",
    "\n",
    "There are many potential use cases for federated learning. For example,\n",
    "\n",
    "- [A scientist training on data from multiple hospitals](https://blog.openmined.org/federated-learning-differential-privacy-and-encrypted-computation-for-medical-imaging/)\n",
    "- [A smartphone app training on data from multiple phones](https://blog.openmined.org/apheris-openmined-pytorch-announcement/)\n",
    "- [A company leaning to predict data when it’s machines need maintenance by training models across data from many sensors](https://blog.openmined.org/predictive-maintenance-of-turbofan-engines-using-federated-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Secure Multi-Party Computation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secure Multi-Party Computation (SMPC) is a different way to encrypt data and then distribute the data to different devices. The main advantage is that, unlike traditional cryptography, SMPC keeps the encrypted data secret yet _alive_, thus allowing us to perform logical and arithmetic operations on the encrypted data.\n",
    "\n",
    "How can you perform math with multi-party computation? Below is a (very) simplified example of how SMPC works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example (see illustration above), we have Andrew holding a piece of sensitive personal data, his special number 5. He can anonymize his data by decomposing the number 5 into two or more different numbers. Imagine that Andrew decides to decompose the number 5 into 2 and 3. This way, he can share his anonymized data with his friends Marianne and Bob.\n",
    "\n",
    "Neither Marianne nor Bob knows the real value of Andrew’s data because they each holds only a part of the special number. And since the decomposed numbers are still _alive_, either of them can perform any kind of operation without seeking the agreement of the other party. Indeed, while these numbers are encrypted between them, we’ll still be able to perform computations. __That way, we can use encrypted values to compute user’s data without showing any kind of sensitive information__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these two concepts, let's now move on to PySyft and PyGrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Use Case: Private Statistical Analysis__\n",
    "Let’s explore the following scenario with two work flows:\n",
    "\n",
    "1. A __data owner__ that wants to publish their sensitive data on their node. (In this case, a hospital pediatric ward).\n",
    "2. A __data scientist__ that wants to find a specific dataset over the grid network to compute some statistical analysis.\n",
    "\n",
    "## __The Data Owner__\n",
    "### __Step 1: Import PySyft and dependencies__\n",
    "To publish the sensitive yet valuable data, the data owner first imports PyGrid and PySyft dependencies.\n",
    "\n",
    "In this example, we’ll import `syft` and wrap the standard `torch` modules with `syft.TorchHook` to add the additional privacy-preserving features that PySyft offers. If you have not set up your computer, follow [this](https://github.com/OpenMined/PyGrid) to install the necessary dependencies.\n",
    "\n",
    "Also, please make sure [these](https://github.com/OpenMined/PyGrid/tree/dev/examples#running-the-examples) instructions have been followed.\n",
    "\n",
    "Once done, you should have the following components:\n",
    "\n",
    " - PyGrid Network      (http://network:7000)\n",
    " - PyGrid Node Bob (http://bob:5001)\n",
    " - PyGrid Node Alice   (http://alice:5000)\n",
    " \n",
    "This tutorial assumes that these components are running in background. See [instructions](https://github.com/OpenMined/PyGrid/tree/dev/examples#how-to-run-this-tutorial) for more details. \n",
    "Yours URL / ports might vary depending on your configuration, so please ensure you look at the output of `docker-compose up` before moving forward.\n",
    "\n",
    "Visit http://localhost:7000/connected-nodes to ensure all nodes are ready and setup! You should get a response like below for the standard setup:\n",
    "\n",
    "`{\"grid-nodes\": [\"Dan\", \"Alice\", \"Charlie\", \"Bob\"]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "from syft.grid.clients.model_centric_fl_client import ModelCentricFLClient\n",
    "\n",
    "import torch as th\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 2: Connect your node__\n",
    "\n",
    "The next step is to connect with your own node. It’s important to note that the node app was deployed in some environment and you need to know its address previously. In this case we’ll connect with the hospital node (Bob) on our `http://localhost:5001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "grid_address = \"http://network:7000\"\n",
    "bob = \"http://bob:5001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_datacluster = DataCentricFLClient(hook, bob)\n",
    "hospital_datacluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 3: Prepare data as tensors and add brief description__\n",
    "\n",
    "Now, let's prepare our dataset to be published on the hospital node. To provide a clear understanding about the data that we publish, we should add a brief description that explains the data and its schema. For this example, let's publish the hospital’s monthly birth records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_description = \"\"\"Description:\n",
    "                        This data presents the monthly birth records.\n",
    "                        \n",
    "                        Columns:\n",
    "                            Gender: 0 - Male, 1 - Female\n",
    "                            Weight: in Kg\n",
    "                            Height: in cm\n",
    "\n",
    "\n",
    "                        Shape 5 * 3\n",
    "                        \"\"\"\n",
    "\n",
    "monthly_birth_records = th.tensor([[1, 3.5, 47.3],\n",
    "                                   [0, 3.7, 48.1],\n",
    "                                   [0, 3.9, 50.0],\n",
    "                                   [1, 4.1, 52.3],\n",
    "                                   [0, 4.1, 49.7]\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 4: Define access rules and permissions__\n",
    "\n",
    "After defining the data schema, we should define the rules that control data access. Here, we’re allowing some users (Bob, Ana and Alice) to get total access to the real values of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_dataset = monthly_birth_records.private_tensor(allowed_users = (\"Bob\", \"Ana\", \"Alice\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 5: Add tags and labels to help data scientists find your dataset__\n",
    "\n",
    "Just like on your favorite social media platform, we need to add some tags to identify and label our data so that our data is more accessible from queries of data scientists who seek specific types of datasets.\n",
    "\n",
    "In this example, we’re adding two tags: __#February__ to identify the month and __#birth-records__ to identify the data meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_dataset = private_dataset.tag(\"#February\", \"#birth-records\").describe(data_description)\n",
    "private_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 6: Publish! You’re done.__\n",
    "\n",
    "Now, the data is ready to be published. It’s important to note that you need to be allowed to publish private datasets on this node. In this example, we’re using Bob’s credential to publish this data on the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pointer = private_dataset.send(hospital_datacluster, user = \"Bob\")\n",
    "data_pointer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data owner, that's all that we need to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __The Data Scientist__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "\n",
    "grid = PublicGridNetwork(hook, grid_address)\n",
    "results = grid.search(\"#February\", \"#birth-records\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 1: Import PySyft and dependencies__\n",
    "\n",
    "As a data scientist, we also need to import the `PySyft` library and wrap `torch` modules using `syft.TorchHook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "\n",
    "import torch as th\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 2: Connect to Grid Platform__\n",
    "\n",
    "Unlike the data owner who published the dataset, the data scientist does not know where the nodes and the datasets are, so we first need to connect with the GridNetwork. The address of the grid network will be the gateway component address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = PublicGridNetwork(hook, grid_address)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 3: What data are you looking for? Search the network.__\n",
    "\n",
    "After connecting with the grid network, we can search for the desired dataset tags. Perhaps you’re looking for x-rays of pneumonia, or hospital birth records. For this tutorial, we’re searching for the same tags that we’ve published before as the data owner. The grid network will return a dictionary containing the node’s ids as keys and data pointers as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid.search(\"#February\", \"#birth-records\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 4: Create a reference to that data pointer__\n",
    "\n",
    "Next, we define a direct reference to the hospital’s data pointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_records = results['Bob'][0]\n",
    "print(feb_records.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Wait - what if I try to copy that data?__\n",
    "\n",
    "If we try to retrieve the real values of the data pointer without being allowed, an exception will be raised. This is how PyGrid can keep the data on the owner’s hands and empower them with the control to allow or deny access to the data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "try:\n",
    "    feb_records.get()\n",
    "except Exception: # GetNotPermittedError\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 5: Perform your computations__\n",
    "\n",
    "Even without copying the data, we are still able to perform remote computations on this data. In this example, we want to compute the average of the babies' weight and height. To do this, we need to sum the column values remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_column(dataset, column):\n",
    "    sum_result = dataset[0][column].copy()\n",
    "    for i in range(1, dataset.shape[0]):\n",
    "        sum_result += dataset[i][column]\n",
    "    return sum_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the weight sum remotely. It will generate other remote tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_sum = sum_column(feb_records, 1)\n",
    "weight_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing with the height column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_sum = sum_column(feb_records, 2)\n",
    "height_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to retrieve the aggregated value using our credentials and divide the value by 5  which is the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_weight = weight_sum.get(user='Bob')/5\n",
    "avg_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing to get the height average. That way, we are able to compute the average weight and height  of the babies that was born on this month without get access to any sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_height = height_sum.get(user='Bob')/5\n",
    "avg_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Done! We know the average height and weight of the babies born in February without ever moving the dataset to our own server, and we never needed to receive any private information about the individual babies.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PyGrid on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PyGrid](https://github.com/OpenMined/PyGrid)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PyGrid/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}